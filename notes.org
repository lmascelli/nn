#+title: Notes on neural networks
#+author: Leonardo Mascelli
#+latex_header: \usepackage{tikz}
#+latex_header: \usetikzlibrary{positioning}
* Network equation
** Simple single neuron network
 
I'll start with a simple network where each layer has only one neuron, one input and one output.

\hfill

Denote:
- $a_0$ is the input of the network,
- $i$ is the $i_{th}$ layer of the network, varing from 1 to N,
- $W_i$ with the $i_{th}$ matrix (in this case scalar) of weights of the $i_{th}$ layer,
- $b_i$ with the $i_{th}$ vector (in this case scalar) of bias of the $i_{th}$ layer,
- $a_i$ with the $i_{th}$ vector (in this case scalar) of outputs of the $i_{th}$ layer,
- $\sigma(x) = \frac{1}{1+e^{-x}}$ so that
  
\begin{equation}
  a_i = \sigma(W_ia_{i-1}+b_i)
\end{equation}

\begin{tikzpicture}[
    inputnode/.style={circle, draw=green},
    outputnode/.style={circle, draw=red},
    comnode/.style={circle, draw=blue},
  ]
  \node[inputnode] (A0)                                {$a_0$};
  \node[comnode]   (W1)        [right= of A0]           {$W_{1}a_{0}$};
  \draw[->] (A0) -- (W1);                         
  \node[comnode]   (B1)        [above= of W1]           {$b_1$};
  \node[outputnode](A1)        [right= of W1]           {$a_1$};
  \draw[->] (W1) -- (A1);                        
  \draw[->] (B1) -- (A1);                         
  \node[comnode]   (W2)        [right= of A1]           {$W_{2}a_{1}$};
  \draw[->] (A1) -- (W2);                         
  \node[comnode]   (B2)        [above= of W2]           {$b_2$};
  \node[outputnode](A2)        [right= of W2]           {$a_2$};
  \draw[->] (W2) -- (A2);                        
  \draw[->] (B2) -- (A2);                         
  \node (dotsnode)             [right=of A2]            {$...$};
  \node[inputnode] (ANm2)      at (0, -5)               {$a_{N-2}$};
  \node[comnode]   (WNm1)      [right= of ANm2]         {$W_{N-1}a_{N-2}$};
  \draw[->] (ANm2) -- (WNm1);                        
  \node[comnode]   (BNm1)      [above= of WNm1]         {$b_{N-1}$};
  \node[outputnode](ANm1)      [right= of WNm1]         {$a_{N-1}$};
  \draw[->] (WNm1) -- (ANm1);                        
  \draw[->] (BNm1) -- (ANm1);                         
  \node[comnode]   (WN)        [right= of ANm1]         {$W_{N}a_{N-1}$};
  \draw[->] (ANm1) -- (WN);                        
  \node[comnode]   (BN)        [above= of WN]           {$b_{N}$};
  \node[outputnode](AN)        [right= of WN]           {$a_{N}$};
  \draw[->] (WN) -- (AN);
  \draw[->] (BN) -- (AN);
\end{tikzpicture}

and denote the cost function

\begin{equation}
  C(W) = \sum_{c=1}^C(a_{N, c} - y_c)^2
\end{equation}

The goal is to find the parameters of the network $W_1, W_2, .. W_n$ that minimize the cost function.

\begin{align}
  \frac{\partial C}{\partial W_1} = \sum_{c=1}^N 2
\end{align}


* Complete network
** Activation function of one layer
\begin{equation}
a_{n}^{k} = \sigma(\sum_{l=1}^{N(k-1)} w_{n, l}^{k} a_{l}^{k-1} + b_{n})
\end{equation}

** Cost function
Let:
- $T$: the number of trials,
- $K$: the number of layers of the network,
- $N(k)$: the number of nodes in the $k$ layer of the network,

\begin{equation}
C = \sum_{i=1}^{T}\sum_{n=1}^{N(K)} (a_{n}^{N(K)} - y_{n})^{2}
\end{equation}

*** Layer K: Last layer
let's try finding the derivatives with the weights of the last layer, $K$ and lets denote the error of the $n_{\text{th}}$ output with:

$\newline$
$\epsilon_{n} = (a_{n}^{K} - y_{n})$

\begin{align}
\frac{\partial C}{\partial w_{c,p}^{K}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}\frac{\partial a_{n}^{K}}{\partial w_{c,p}^{K}}
\\
\frac{\partial C}{\partial w_{c,p}^{K}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}\frac{\partial \sigma(\sum_{l=1}^{N(K-1)} w_{n, l}^{K} a_{l}^{K-1} + b_{n})}{\partial w_{c,p}^{K}}
\\
\frac{d \sigma(x)}{dx} &= \sigma(x)(1 - \sigma(x))
\\
z_{n}^{K} &= \sum_{l=1}^{N(K-1)} w_{n, l}^{K} a_{l}^{K-1} + b_{n}
\\
\frac{\partial C}{\partial w_{c,p}^{K}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})\frac{\partial z_{n}^{K}}{\partial w_{c,p}^{K}}
\\
\frac{\partial C}{\partial w_{c,p}^{K}} &= \sum_{i=1}^{T}2\epsilon_{c}a_{c}^{K}(1-a_{c}^{K})a_{p}^{K-1}
\end{align}

We've found the the derivatives for the weights of the last layer are:
$\newline$
\begin{equation}
\frac{\partial C}{\partial w_{c,p}^{K}} = \sum_{i=1}^{T}2\epsilon_{c}a_{c}^{K}(1-a_{c}^{K})a_{p}^{K-1}
\end{equation}
$\newline$
Let's define $2\epsilon_{c}a_{c}^{K}(1-a_{c}^{K})$, the back propagation of the error in $c$ node of the last layer as $e_{c}^{K}$.
Then:
$\newline$
\begin{equation}
\frac{\partial C}{\partial w_{c,p}^{K}} = \sum_{i=1}^{T}e_{c} a_{p}
\end{equation}

*** Layer K - 1
let's try now with the derivatives of the layer before the last, $K-1$: 
\begin{align}
\frac{\partial C}{\partial w_{c,p}^{K-1}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}\frac{\partial a_{n}^{K}}{\partial w_{c,p}^{K-1}}
\\
\frac{\partial C}{\partial w_{c,p}^{K-1}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}\frac{\partial \sigma(\sum_{l=1}^{N(K-1)} w_{n, l}^{K} a_{l}^{K-1} + b_{n})}{\partial w_{c,p}^{K-1}}
\\
z_{n}^{K} &= \sum_{l=1}^{N(K-1)} w_{n, l}^{K} a_{l}^{K-1} + b_{n}
\\
\frac{\partial C}{\partial w_{c,p}^{K-1}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})\frac{\partial z_{n}^{K}}{\partial w_{c,p}^{K-1}}
\\
\frac{\partial z_{n}^{K}}{\partial w_{c,p}^{K-1}} &= \frac{\partial \sum_{l=1}^{N(K-1)} w_{n, l}^{K} a_{l}^{K-1} + b_{n}}{\partial w_{c,p}^{K-1}} = \sum_{l=1}^{N(K-1)} w_{n, l}^{K}\frac{\partial a_{l}^{K-1}}{\partial w_{c,p}^{K-1}}
\\
z_{l}^{K-1} &= \sum_{m=1}^{N(K-2)} w_{l, m}^{K-1} a_{m}^{K-2} + b_{l}
\\
\frac{\partial z_{n}^{K}}{\partial w_{c,p}^{K-1}} &= \sum_{l=1}^{N(K-1)} w_{n, l}^{K}a_{l}^{K-1}(1-a_{l}^{K-1})\frac{\partial z_{l}^{K-1}}{\partial w_{c,p}^{K-1}}
\\
\frac{\partial z_{n}^{K-1}}{\partial w_{c,p}^{K-1}} &= a_{p}^{K-2}
\\
\frac{\partial z_{n}^{K}}{\partial w_{c,p}^{K-1}} &= w_{n, c}^{K}a_{c}^{K-1}(1-a_{c}^{K-1})a_{p}^{K-2}
\\
\frac{\partial C}{\partial w_{c,p}^{K-1}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})w_{n, c}^{K}a_{c}^{K-1}(1-a_{c}^{K-1})a_{p}^{K-2}
\end{align}


We've found the the derivatives for the weights of layer before the last layer are:
\begin{equation}
\frac{\partial C}{\partial w_{c,p}^{K-1}} = \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})w_{n, c}^{K}a_{c}^{K-1}(1-a_{c}^{K-1})a_{p}^{K-2}
\end{equation}

You can recognize in the equation the term $2e_{n}a_{n}^{K}(1-a_{n}^{K})$ to be what we first had defined as the propagation of the error in the $K$ layer, $e_{n}^{K}$.

\begin{equation}
\frac{\partial C}{\partial w_{c,p}^{K-1}} = \sum_{i=1}^{T}\sum_{n=1}^{N(K)}e_{n}^{K}w_{n, c}a_{c}^{K-1}(1-a_{c}^{K-1})a_{p}^{K-2}
\end{equation}

and define
\begin{equation}
e_{c}^{K-1} = \sum_{i=1}^{T}\sum_{n=1}^{N(K)}e_{n}^{K}w_{n, c}a_{c}^{K-1}(1-a_{c}^{K-1})
\end{equation}

so that:
\begin{equation}
\frac{\partial C}{\partial w_{c,p}^{K-1}} = \sum_{i=1}^{T}e_c^{K-1}a_{p}^{K-2}
\end{equation}

*** Layer K-2
let's try now with the derivatives of two layers before the last, $K-2$:
\begin{align}
\frac{\partial C}{\partial w_{c,p}^{K-2}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}\frac{\partial a_{n}^{K}}{\partial w_{c,p}^{K-2}}
\\
\frac{\partial C}{\partial w_{c,p}^{K-2}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}\frac{\partial \sigma(\sum_{l=1}^{N(K-1)} w_{n, l}^{K} a_{l}^{K-1} + b_{n})}{\partial w_{c,p}^{K-2}}
\\
\frac{\partial C}{\partial w_{c,p}^{K-2}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})\frac{\partial (\sum_{l=1}^{N(K-1)} w_{n, l}^{K} a_{l}^{K-1} + b_{n})}{\partial w_{c,p}^{K-2}}
\\
\frac{\partial C}{\partial w_{c,p}^{K-2}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})(\sum_{l=1}^{N(K-1)} w_{n, l}^{K} \frac{\partial a_{l}^{K-1}}{\partial w_{c,p}^{K-2}})
\\
\frac{\partial C}{\partial w_{c,p}^{K-2}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})(\sum_{l=1}^{N(K-1)} w_{n, l}^{K}a_{l}^{K-1}(1-a_{l}^{K-1})\frac{\partial \sum_{m=1}^{N(K-2)} w_{l,m}^{K-1}a_{m}^{K-2} + b_{m}^{K-1}}{\partial w_{c,p}^{K-2}})
\\
\frac{\partial C}{\partial w_{c,p}^{K-2}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})(\sum_{l=1}^{N(K-1)} w_{n, l}^{K}a_{l}^{K-1}(1-a_{l}^{K-1})\sum_{m=1}^{N(K-2)} w_{l,m}^{K-1}\frac{\partial a_{m}^{K-2}}{\partial w_{c,p}^{K-2}})
\\
\begin{split}
\frac{\partial C}{\partial w_{c,p}^{K-2}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K}) \sum_{l=1}^{N(K-1)} w_{n, l}^{K}a_{l}^{K-1}(1-a_{l}^{K-1})\sum_{m=1}^{N(K-2)} w_{l,m}^{K-1}a_{m}^{K-2}(1-a_{m}^{K-2}) \\ &\frac{\partial \sum_{o=1}^{N(K-3)} w_{m,o}^{K-2} a_{o}^{K-3} + b_{m}^{K-2}}{\partial w_{c,p}^{K-2}}
\end{split}
\\
\frac{\partial C}{\partial w_{c,p}^{K-2}} &= \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})\sum_{l=1}^{N(K-1)} w_{n, l}^{K}a_{l}^{K-1}(1-a_{l}^{K-1})w_{l,c}^{K-1}a_{c}^{K-2}(1-a_{c}^{K-2})a_{p}^{K-3}
\end{align}

So the derivative of the layer $K-2$ is:

\begin{equation}
\frac{\partial C}{\partial w_{c,p}^{K-2}} = \sum_{i=1}^{T}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})\sum_{l=1}^{N(K-1)} w_{n, l}^{K}a_{l}^{K-1}(1-a_{l}^{K-1})w_{l,c}^{K-1}a_{c}^{K-2}(1-a_{c}^{K-2})a_{p}^{K-3}
\end{equation}

switching the summatories with $n$ and $l$:
\begin{equation}
\frac{\partial C}{\partial w_{c,p}^{K-2}} = \sum_{i=1}^{T}\sum_{l=1}^{N(K-1)}\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})w_{n, l}^{K}a_{l}^{K-1}(1-a_{l}^{K-1})w_{l,c}^{K-1}a_{c}^{K-2}(1-a_{c}^{K-2})a_{p}^{K-3}
\end{equation}

you can see that the term

\begin{equation}
\sum_{n=1}^{N(K)} 2\epsilon_{n}a_{n}^{K}(1-a_{n}^{K})w_{n, l}^{K}a_{l}^{K-1}(1-a_{l}^{K-1})
\end{equation}

is the propagation of the error to the $l$ element of the $K-1$ layer, $e_{l}^{K-1}$. Then you can write:

\begin{equation}
\frac{\partial C}{\partial w_{c,p}^{K-2}} = \sum_{i=1}^{T}\sum_{l=1}^{N(K-1)}e_{l}^{K-1}w_{l,c}^{K-1}a_{c}^{K-2}(1-a_{c}^{K-2})a_{p}^{K-3}
\end{equation}

and denote

\begin{equation}
e_{c}^{K-2} = \sum_{l=1}^{N(K-1)}e_{l}^{K-1}w_{l,c}^{K-1}a_{c}^{K-2}(1-a_{c}^{K-2})
\end{equation}

as the propagation of the error to the $c$ node of the $K-2$ layer so that:

\begin{equation}
\frac{\partial C}{\partial w_{c,p}^{K-2}} = e_{c}^{K-2}a_{p}^{K-3}
\end{equation}
